#!/usr/bin/env python3
"""
1ì°¨ íŒŒì´í”„ë¼ì¸: ë² ì´ìŠ¤ëª¨ë¸ â†’ FT/ë°ì´í„°ì…‹ â†’ .pth
FT ì˜¬ì¸ì› í”„ë ˆì„ì›Œí¬ + TSEL ì‚¬ê³ ê¸°ë²• + H100 ê·¹í•œ ìµœì í™” + 24ë§Œì ë°ì´í„°ì…‹ ì™„ì „ í™œìš©
"""

import os
import torch
import json
import random
import logging
from pathlib import Path
from typing import List, Dict, Tuple
from torch.utils.data import Dataset, DataLoader
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ê°•ì œ ì„¤ì •
os.environ["TORCH_LOAD_SAFE_MODE"] = "0"
os.environ["TORCH_USE_CUDA_DSA"] = "0"
os.environ["TF_USE_LEGACY_KERAS"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "0"
os.environ["HF_HUB_OFFLINE"] = "0"
os.environ["TRANSFORMERS_USE_SAFETENSORS"] = "0"  # pytorch_model.bin ì‚¬ìš©

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    TrainingArguments,
    Trainer,
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)


class TranscendentalDialogueDataset(Dataset):
    """24ë§Œì ì´ˆì›”ëŒ€í™” ë°ì´í„°ì…‹ í´ë˜ìŠ¤ - ëª¨ë¸ ë°ì´í„°ì— ìµœì í™”"""

    def __init__(self, data_pairs: List[Dict], tokenizer, max_length: int = 1024):
        self.data_pairs = data_pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

        # NLLB-200 ëª¨ë¸ íŠ¹ì„±ì— ë§ì¶˜ ì„¤ì • (ì‹¤ì œ ëª¨ë¸ ê¸°ë°˜)
        self.model_config = {
            "d_model": 1024,
            "encoder_layers": 24,  # ì‹¤ì œ ëª¨ë¸: 24
            "decoder_layers": 24,  # ì‹¤ì œ ëª¨ë¸: 24
            "encoder_attention_heads": 16,
            "decoder_attention_heads": 16,
            "encoder_ffn_dim": 8192,  # ì‹¤ì œ ëª¨ë¸: 8192
            "decoder_ffn_dim": 8192,  # ì‹¤ì œ ëª¨ë¸: 8192
            "max_position_embeddings": 1024,
            "vocab_size": 256206,  # ì‹¤ì œ ëª¨ë¸: 256206
            "attention_dropout": 0.0,
            "activation_dropout": 0.0,
            "dropout": 0.1,
            "scale_embedding": True,
            "init_std": 0.02,
        }

        # í•œêµ­ì–´ ë³µì¡ì„± íŠ¹ì„± (24ë§Œì ë°ì´í„°ì…‹ í™œìš©)
        self.korean_complexity_features = {
            "structural_complexity": "world_highest",
            "precise_expression": "nuance_capture",
            "rich_vocabulary": "diverse_expression",
            "metaphorical_thinking": "abstract_concept",
            "context_dependency": "high_understanding",
            "tsel_thinking": "transcendental_logic",
            "origin_echo_specialization": "unique_dialogue",
        }

        logger.info(f"[ğŸ“Š] 24ë§Œì ì´ˆì›”ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ: {len(data_pairs)}ê°œ ìƒ˜í”Œ")
        logger.info(f"[ğŸ‡°ğŸ‡·] í•œêµ­ì–´ ë³µì¡ì„± íŠ¹ì„±: {self.korean_complexity_features}")
        logger.info(f"[ğŸ”§] ëª¨ë¸ ì„¤ì •: {self.model_config}")

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        pair = self.data_pairs[idx]

        # 24ë§Œì ë°ì´í„°ì…‹ íŠ¹ì„± ê°•í™”
        source_text = self._enhance_with_transcendental_features(pair["instruction"])
        target_text = self._enhance_with_transcendental_features(pair["response"])

        # NLLB-200 í† í¬ë‚˜ì´ì € ìµœì í™”
        source_encoding = self.tokenizer(
            source_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        target_encoding = self.tokenizer(
            target_text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )

        # ëª¨ë¸ íŠ¹ì„±ì— ë§ì¶˜ ìœ„ì¹˜ ì„ë² ë”© ìµœì í™”
        attention_mask = source_encoding["attention_mask"].squeeze()
        if attention_mask.shape[0] > self.model_config["max_position_embeddings"]:
            attention_mask = attention_mask[
                : self.model_config["max_position_embeddings"]
            ]
            source_encoding["input_ids"] = source_encoding["input_ids"][
                :, : self.model_config["max_position_embeddings"]
            ]

        return {
            "input_ids": source_encoding["input_ids"].squeeze(),
            "attention_mask": attention_mask,
            "labels": target_encoding["input_ids"].squeeze(),
        }

    def _enhance_with_transcendental_features(self, text: str) -> str:
        """24ë§Œì ì´ˆì›”ëŒ€í™” íŠ¹ì„± ê°•í™”"""

        # TSEL ì‚¬ê³ ê¸°ë²• íŠ¹ì„± ì¶”ê°€
        enhanced_text = f"[TSEL:0<1>0] {text} [ORIGIN-ECHO:TRANSCENDENTAL]"

        # í•œêµ­ì–´ ë³µì¡ì„± íŠ¹ì„± ë°˜ì˜
        if any(korean_char in text for korean_char in "ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜"):
            enhanced_text += " [KOREAN:COMPLEXITY:MAXIMUM]"

        return enhanced_text


# FasterTransformer ê´€ë ¨ í•¨ìˆ˜ ì œê±° (ê¸°ë³¸ ëª¨ë¸ ìµœì í™”ë¡œ ëŒ€ì²´)


def apply_ft_optimization(model, tokenizer):
    """FasterTransformer ê·¹í•œ ìµœì í™” ì ìš©"""

    logger.info("âš¡ FasterTransformer ê·¹í•œ ìµœì í™” ì ìš©")

    # ì»¤ë„.so íŒŒì¼ë“¤ í™•ì¸
    ft_paths = [
        "FasterTransformer/build/lib/libTransformerTritonBackend.so",
        "FasterTransformer/build/lib/libtransformer-shared.so",
    ]

    for path in ft_paths:
        if os.path.exists(path):
            logger.info(f"âœ… ì»¤ë„.so ë°œê²¬: {path}")
        else:
            logger.warning(f"âš ï¸ ì»¤ë„.so ì—†ìŒ: {path}")

    # Python ë°”ì¸ë”© ë¡œë“œ ì‹œë„
    try:
        import fastertransformer as ft

        logger.info("âœ… FasterTransformer Python ë°”ì¸ë”© ë¡œë“œ ì„±ê³µ")

        # FT ìµœì í™” ì ìš©
        model = apply_ft_kernel_optimization(model)
        logger.info("âœ… FasterTransformer ì»¤ë„ ìµœì í™” ì ìš© ì™„ë£Œ")

    except ImportError as e:
        logger.error(f"âŒ FasterTransformer Python ë°”ì¸ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise Exception("FasterTransformer Python ë°”ì¸ë”©ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    return model


def apply_ft_kernel_optimization(model):
    """FT ì»¤ë„ ìµœì í™” ì ìš©"""

    logger.info("ğŸ”§ FT ì»¤ë„ ìµœì í™” ì ìš©")

    # NLLB-200 ëª¨ë¸ íŠ¹ì„±ì— ë§ì¶˜ FT ì„¤ì •
    ft_config = {
        "max_batch_size": 16,  # H100 ìµœì í™”
        "max_seq_len": 1024,  # ëª¨ë¸ ìµœëŒ€ ê¸¸ì´
        "num_heads": 16,  # ì–´í…ì…˜ í—¤ë“œ ìˆ˜
        "hidden_size": 1024,  # d_model
        "inter_size": 8192,  # ffn_dim (ì‹¤ì œ ëª¨ë¸)
        "num_layers": 24,  # ë ˆì´ì–´ ìˆ˜ (ì‹¤ì œ ëª¨ë¸)
        "vocab_size": 256206,  # ì–´íœ˜ í¬ê¸° (ì‹¤ì œ ëª¨ë¸)
        "data_type": "FP16",  # H100 FP16 ê°€ì†
        "tensor_para_size": 1,
        "pipeline_para_size": 1,
    }

    logger.info(f"ğŸ”§ FT ê·¹í•œ ì„¤ì •: {ft_config}")

    # ëª¨ë¸ ê·¹í•œ ìµœì í™” ì ìš©
    for name, module in model.named_modules():
        if "attention" in name.lower():
            # ì–´í…ì…˜ ëª¨ë“ˆ ê·¹í•œ ìµœì í™”
            if hasattr(module, "num_attention_heads"):
                module.num_attention_heads = min(module.num_attention_heads, 16)
            logger.info(f"âš¡ ì–´í…ì…˜ ëª¨ë“ˆ ê·¹í•œ ìµœì í™”: {name}")

    # ë©”ëª¨ë¦¬ ê·¹í•œ ìµœì í™”
    if hasattr(model, "config"):
        model.config.use_cache = False
        model.config.gradient_checkpointing = True

    logger.info("âœ… FasterTransformer ê·¹í•œ ìµœì í™” ì™„ë£Œ")
    return model


def apply_basic_optimization(model, tokenizer):
    """ê¸°ë³¸ ëª¨ë¸ ìµœì í™” (FT ì œê±°)"""

    logger.info("âš¡ ê¸°ë³¸ ëª¨ë¸ ìµœì í™” ì ìš©")

    # 1. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.eval()

    # 2. ë©”ëª¨ë¦¬ ìµœì í™”
    if torch.cuda.is_available():
        model = model.cuda()
        torch.cuda.empty_cache()
        logger.info("âœ… CUDA ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ")

    # 3. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
    if hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()
        logger.info("âœ… ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”")

    # 4. ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ë³µì›
    model.train()

    logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
    return model


def create_evaluation_dataset(
    dataset_path: str = "data/dialogue_dataset_100k.jsonl",
    eval_ratio: float = 0.1,
    output_path: str = "data/eval_dataset.jsonl",
):
    """24ë§Œì ë°ì´í„°ì…‹ì—ì„œ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±"""

    logger.info(f"[ğŸ“Š] 24ë§Œì í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘")
    logger.info(f"[ğŸ“¥] ì›ë³¸ ë°ì´í„°ì…‹: {dataset_path}")
    logger.info(f"[ğŸ“¤] í‰ê°€ ë°ì´í„°ì…‹: {output_path}")
    logger.info(f"[ğŸ“ˆ] í‰ê°€ ë¹„ìœ¨: {eval_ratio * 100}%")

    # 24ë§Œì ë°ì´í„°ì…‹ ë¡œë“œ
    with open(dataset_path, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]

    logger.info(f"[ğŸ“Š] ì „ì²´ 24ë§Œì ë°ì´í„°: {len(data)}ê°œ")

    # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
    random.shuffle(data)
    eval_size = int(len(data) * eval_ratio)
    eval_data = data[:eval_size]
    train_data = data[eval_size:]

    logger.info(f"[ğŸ“Š] í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    logger.info(f"[ğŸ“Š] í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ")

    # í‰ê°€ ë°ì´í„°ì…‹ ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        for item in eval_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    # í•™ìŠµìš© ë°ì´í„°ì…‹ë„ ì—…ë°ì´íŠ¸
    train_output_path = "data/train_dataset.jsonl"
    with open(train_output_path, "w", encoding="utf-8") as f:
        for item in train_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    logger.info(f"[âœ…] 24ë§Œì í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ")
    logger.info(f"[ğŸ“] í•™ìŠµ ë°ì´í„°ì…‹: {train_output_path}")
    logger.info(f"[ğŸ“] í‰ê°€ ë°ì´í„°ì…‹: {output_path}")

    return train_output_path, output_path


def load_transcendental_dataset(dataset_path: str) -> List[Dict]:
    """24ë§Œì ì´ˆì›”ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ"""

    logger.info(f"[ğŸ“š] 24ë§Œì ì´ˆì›”ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ: {dataset_path}")

    with open(dataset_path, "r", encoding="utf-8") as f:
        data = [json.loads(line) for line in f]

    logger.info(f"[ğŸ“Š] ë¡œë“œëœ 24ë§Œì ìƒ˜í”Œ: {len(data)}ê°œ")

    # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    valid_data = []
    for item in data:
        if "instruction" in item and "response" in item:
            if len(item["instruction"]) > 10 and len(item["response"]) > 10:
                valid_data.append(item)

    logger.info(f"[âœ…] ìœ íš¨í•œ 24ë§Œì ìƒ˜í”Œ: {len(valid_data)}ê°œ")
    return valid_data


def finetune_with_h100_optimization(model, tokenizer, data_pairs):
    """H100 ê·¹í•œ ìµœì í™” + 24ë§Œì ë°ì´í„°ì…‹ íŒŒì¸íŠœë‹"""

    logger.info("[ğŸš€] H100 ê·¹í•œ ìµœì í™” + 24ë§Œì ë°ì´í„°ì…‹ íŒŒì¸íŠœë‹ ì‹œì‘")

    # í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
    train_path, eval_path = create_evaluation_dataset()

    # í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ
    train_data = load_transcendental_dataset(train_path)
    eval_data = load_transcendental_dataset(eval_path)

    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = TranscendentalDialogueDataset(train_data, tokenizer, 1024)
    eval_dataset = TranscendentalDialogueDataset(eval_data, tokenizer, 1024)

    # H100 ê·¹í•œ ìµœì í™” ì„¤ì • (model_atomic_info.json ê¸°ë°˜)
    training_args = TrainingArguments(
        output_dir="./fillinlapp_h100_model",
        num_train_epochs=3,  # 24ë§Œì ë°ì´í„°ì…‹ ì™„ì „ í™œìš©
        per_device_train_batch_size=8,  # ì‹¤ì œ ëª¨ë¸ í¬ê¸°ì— ë§ì¶˜ ì¡°ì •
        per_device_eval_batch_size=8,
        warmup_steps=300,  # 24ë§Œì ë°ì´í„°ì…‹ì— ë§ì¶˜ ì›Œë°ì—…
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=50,
        save_steps=500,
        eval_steps=500,
        save_strategy="steps",
        eval_strategy="steps",  # í‰ê°€ ë°ì´í„°ì…‹ í™œìš©
        load_best_model_at_end=True,  # ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,  # H100ì—ì„œëŠ” BF16ì´ ë” ì¢‹ìŒ
        bf16=True,  # H100 ë„¤ì´í‹°ë¸Œ BF16 ì§€ì› (FP16ë³´ë‹¤ ì •í™•)
        dataloader_num_workers=8,  # H100 ê·¹í•œ ìµœì í™”
        gradient_accumulation_steps=8,  # ì‹¤ì œ ëª¨ë¸ í¬ê¸°ì— ë§ì¶˜ ì¡°ì •
        learning_rate=5e-5,  # H100 ê·¹í•œ ìµœì í™”
        save_total_limit=2,  # ë©”ëª¨ë¦¬ ì ˆì•½
        remove_unused_columns=False,
        push_to_hub=False,
        report_to=None,
        gradient_checkpointing=True,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        dataloader_drop_last=True,  # ì•ˆì •ì„±
        group_by_length=True,  # ì–´í…ì…˜ ìµœì í™”
        optim="adamw_torch_fused",  # H100 ìµœì í™”ëœ ì˜µí‹°ë§ˆì´ì €
        # 24ë§Œì ë°ì´í„°ì…‹ íŠ¹í™” ì„¤ì •
        max_grad_norm=1.0,  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        lr_scheduler_type="cosine",  # ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬
        warmup_ratio=0.1,  # ì›Œë°ì—… ë¹„ìœ¨
    )

    # TSEL ì‚¬ê³ ê¸°ë²• ì§„í™” ëª©í‘œ
    evolution_targets = {
        "translation_quality": "maximum",
        "language_mastery": "evolved",
        "context_understanding": "enhanced",
        "metaphorical_translation": "enabled",
        "all_language_improvement": "guaranteed",
        "tsel_thinking_integration": "complete",
        "korean_complexity_utilization": "maximum",
        "origin_echo_specialization": "perfect",
    }

    logger.info(f"[ğŸ¯] TSEL ì§„í™” ëª©í‘œ: {evolution_targets}")

    # Trainer ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # í‰ê°€ ë°ì´í„°ì…‹ í™œìš©
        tokenizer=tokenizer,
    )

    # H100 ê·¹í•œ ìµœì í™” íŒŒì¸íŠœë‹ ì‹¤í–‰
    logger.info("[ğŸš€] H100 ê·¹í•œ ìµœì í™” íŒŒì¸íŠœë‹ ì‹œì‘")
    trainer.train()

    logger.info("[âœ…] H100 ê·¹í•œ ìµœì í™” íŒŒì¸íŠœë‹ ì™„ë£Œ")

    return model


def save_model_to_pth(
    model, tokenizer, output_path: str = "llmmodel/pytorch/tsel_finetuned.pth"
):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ .pth íŒŒì¼ë¡œ ì €ì¥"""

    logger.info(f"[ğŸ’¾] íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ .pth íŒŒì¼ë¡œ ì €ì¥: {output_path}")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)

    # ëª¨ë¸ ìƒíƒœ ì €ì¥
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "config": model.config,
            "tokenizer": tokenizer,
            "model_info": {
                "model_type": "nllb-200-distilled-1.3b",
                "finetuned": True,
                "tsel_thinking": True,
                "korean_complexity": True,
                "h100_optimized": True,
                "dataset_size": "240k_transcendental_dialogue",
                "training_epochs": 3,
                "optimization_level": "extreme",
            },
        },
        output_path,
    )

    logger.info(f"[âœ…] .pth íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
    logger.info(f"[ğŸ“Š] .pth íŒŒì¼ í¬ê¸°: {file_size:.2f}MB")


def main():
    """1ì°¨ íŒŒì´í”„ë¼ì¸ ë©”ì¸ í•¨ìˆ˜"""

    logger.info("=" * 80)
    logger.info("ğŸš€ 1ì°¨ íŒŒì´í”„ë¼ì¸: ë² ì´ìŠ¤ëª¨ë¸ â†’ FT/ë°ì´í„°ì…‹ â†’ .pth")
    logger.info("ğŸ¯ ëª©í‘œ: H100 ê·¹í•œ ìµœì í™” + 24ë§Œì ë°ì´í„°ì…‹ ì™„ì „ í™œìš©")
    logger.info("ğŸ§  ì‚¬ê³ ê¸°ë²•: TSEL (ì •ë°©í–¥ + ì—­ë°©í–¥ + í†µí•©)")
    logger.info("=" * 80)

    # STEP 1: ë² ì´ìŠ¤ëª¨ë¸ ë¡œë“œ
    logger.info("[ğŸ“¥] STEP 1: NLLB-200-distilled-1.3B ë² ì´ìŠ¤ëª¨ë¸ ë¡œë“œ")

    # ì»¨í…Œì´ë„ˆ ë‚´ ì •í™•í•œ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_paths = [
        "llmmodel/nllb-200-1.3B",  # ì‹¤ì œ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸
        "llmmodel/nllb-200-distilled-1.3B",  # ê¸°ì¡´ ê²½ë¡œ (fallback)
        "llmmodel/models--facebook--nllb-200-distilled-1.3B",
    ]

    actual_model_path = None
    for path in model_paths:
        if path.endswith("*"):
            # glob íŒ¨í„´ ì²˜ë¦¬
            import glob

            matches = glob.glob(path)
            if matches:
                actual_model_path = matches[0]
                break
        elif os.path.exists(path):
            actual_model_path = path
            break

    if actual_model_path:
        logger.info(f"[âœ…] ì»¨í…Œì´ë„ˆ ë‚´ ëª¨ë¸ ê²½ë¡œ ë°œê²¬: {actual_model_path}")

        # ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(actual_model_path)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            actual_model_path,
            use_safetensors=False,  # pytorch_model.bin ì‚¬ìš©
        )
    else:
        logger.info("[ğŸ“¥] ë¡œì»¬ ëª¨ë¸ ì—†ìŒ - HuggingFaceì—ì„œ ì§ì ‘ ë¡œë“œ")

        # HuggingFaceì—ì„œ ì§ì ‘ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-1.3B")
        model = AutoModelForSeq2SeqLM.from_pretrained(
            "facebook/nllb-200-distilled-1.3B",
            use_safetensors=False,  # pytorch_model.bin ì‚¬ìš©
        )

    logger.info(f"[âœ…] ë² ì´ìŠ¤ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # STEP 2: FT ì˜¬ì¸ì› í”„ë ˆì„ì›Œí¬ ì ìš©
    logger.info("[âš¡] STEP 2: FT ì˜¬ì¸ì› í”„ë ˆì„ì›Œí¬ ê·¹í•œ ìµœì í™” ì ìš©")

    # FT ìµœì í™” ì‹œë„ (ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
    try:
        model = apply_ft_optimization(model, tokenizer)
        logger.info("[âœ…] FT ê·¹í•œ ìµœì í™” ì ìš© ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"[âš ï¸] FT ìµœì í™” ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: {e}")
        model = apply_basic_optimization(model, tokenizer)
        logger.info("[âœ…] ê¸°ë³¸ ëª¨ë¸ë¡œ ì§„í–‰")

    # STEP 3: 24ë§Œì ì´ˆì›”ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ
    logger.info("[ğŸ“š] STEP 3: 24ë§Œì ì´ˆì›”ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ")

    data_pairs = load_transcendental_dataset("data/dialogue_dataset_100k.jsonl")

    logger.info(f"[âœ…] 24ë§Œì ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(data_pairs)}ê°œ ìƒ˜í”Œ")
    logger.info(f"[ğŸ“Š] ë°ì´í„°ì…‹ í¬ê¸°: {len(data_pairs)}ê°œ (ì˜ˆìƒ: 24ë§Œê°œ)")

    if len(data_pairs) < 1000:
        logger.warning(f"[âš ï¸] ë°ì´í„°ì…‹ì´ ì‘ìŠµë‹ˆë‹¤. ì‹¤ì œ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        logger.info(f"[ğŸ“] ë°ì´í„°ì…‹ íŒŒì¼: data/dialogue_dataset_100k.jsonl")

    # STEP 4: H100 ê·¹í•œ ìµœì í™” íŒŒì¸íŠœë‹
    logger.info("[ğŸ¯] STEP 4: H100 ê·¹í•œ ìµœì í™” + TSEL ì‚¬ê³ ê¸°ë²• íŒŒì¸íŠœë‹")

    finetuned_model = finetune_with_h100_optimization(model, tokenizer, data_pairs)

    logger.info("[âœ…] H100 ê·¹í•œ ìµœì í™” íŒŒì¸íŠœë‹ ì™„ë£Œ")

    # STEP 5: .pth íŒŒì¼ ì €ì¥
    logger.info("[ğŸ’¾] STEP 5: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ .pth íŒŒì¼ë¡œ ì €ì¥")

    save_model_to_pth(finetuned_model, tokenizer)

    logger.info("[âœ…] .pth íŒŒì¼ ì €ì¥ ì™„ë£Œ")

    # ì™„ë£Œ ë©”ì‹œì§€
    logger.info("=" * 80)
    logger.info("ğŸ‰ 1ì°¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info("ğŸ“Š H100 ê·¹í•œ ìµœì í™” ì™„ë£Œ!")
    logger.info("ğŸ‡°ğŸ‡· 24ë§Œì ë°ì´í„°ì…‹ ì™„ì „ í™œìš© ì™„ë£Œ!")
    logger.info("ğŸ§  TSEL ì‚¬ê³ ê¸°ë²• ì ìš© ì™„ë£Œ!")
    logger.info("ğŸ’¾ .pth íŒŒì¼ ìƒì„± ì™„ë£Œ!")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
